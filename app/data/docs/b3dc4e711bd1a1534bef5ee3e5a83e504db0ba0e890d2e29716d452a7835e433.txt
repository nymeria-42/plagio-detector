O risco existencial da inteligência artificial geral é a hipótese de que o progresso substancial da inteligência artificial generativa (IAG) poderia resultar na extinção humana ou em alguma outra catástrofe global irrecuperável.
A escola do risco existencial ("risco x") argumenta da seguinte forma: atualmente, a espécie humana domina outras espécies porque o cérebro humano tem algumas capacidades distintas que outros animais não têm. Se a IA superar a humanidade em termos de inteligência geral e se tornar "superinteligente", poderá ser difícil ou impossível controlá-la. Assim como o destino do gorila das montanhas depende da boa vontade humana, o destino da humanidade pode depender das ações de uma futura máquina superinteligente.
A probabilidade desse tipo de cenário é amplamente debatida e depende, em parte, de diferentes cenários para o progresso futuro da ciência da computação. As preocupações com a superinteligência foram expressas pelos principais cientistas da computação e CEOs de tecnologia, como Geoffrey Hinton, Yoshua Bengio, Alan Turing, Elon Musk, e Sam Altman (CEO da OpenAI). Em 2022, uma pesquisa com pesquisadores de IA revelou que alguns pesquisadores acreditam que há uma chance de 10% ou mais de que nossa incapacidade de controlar a IA cause uma catástrofe existencial (mais da metade dos entrevistados da pesquisa, com uma taxa de resposta de 17%).
Duas fontes de preocupação são os problemas de controle e alinhamento da IA: controlar uma máquina superinteligente ou incutir nela valores compatíveis com os humanos pode ser um problema mais difícil do que se supõe ingenuamente. Muitos pesquisadores acreditam que uma superinteligência resistiria a tentativas de desligamento ou mudança de seus objetivos (pois tal incidente a impediria de atingir seus objetivos atuais) e que será extremamente difícil alinhar a superinteligência com toda a amplitude de valores e restrições humanas importantes. Em contrapartida, céticos como o cientista da computação Yann LeCun argumentam que as máquinas superinteligentes não terão desejo de autopreservação.
Uma terceira fonte de preocupação é que uma "explosão de inteligência" repentina pode pegar de surpresa uma raça humana despreparada. Para ilustrar, se a primeira geração de um programa de computador capaz de corresponder amplamente à eficácia de um pesquisador de IA puder reescrever seus algoritmos e dobrar sua velocidade ou seus recursos em seis meses, espera-se que o programa de segunda geração leve três meses para realizar uma parte semelhante do trabalho. Nesse cenário, o tempo para cada geração continua a diminuir, e o sistema passa por um número sem precedentes de gerações de aprimoramento em um curto intervalo de tempo, saltando de um desempenho sub-humano em muitas áreas para um desempenho sobre-humano em praticamente todos os domínios de interesse. Empiricamente, exemplos como o AlphaZero no domínio do Go mostram que os sistemas de IA podem, às vezes, progredir de uma capacidade estreita de nível humano para uma capacidade estreita sobre-humana de forma extremamente rápida.