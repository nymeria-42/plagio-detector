Uma inteligência artificial explicável (IAE, IA explicável, ou XAI) é uma inteligência artificial (IA) em que os resultados da solução podem ser compreendidos por humanos. Isso contrasta com o conceito de "caixa preta" no aprendizado de máquina, em que nem mesmo seus designers podem explicar por que uma IA chegou a uma decisão específica. A IAE pode ser uma implementação do direito à explicação social. A IAE é relevante mesmo se não houver nenhum direito legal ou requisito regulatório—por exemplo, a IAE pode melhorar a experiência do usuário de um produto ou serviço ajudando os usuários finais a confiar que a IA está tomando boas decisões. Assim, o objetivo da IAE é explicar o que foi feito, o que é feito agora, o que será feito a seguir e desvendar as informações nas quais as ações se baseiam. Essas características permitem (i) confirmar o conhecimento existente (ii) contestar o conhecimento existente e (iii) gerar novas suposições.
Os algoritmos usados em IA podem ser separados entre algoritmos de aprendizado de máquina (ML) de caixa branca e caixa preta. Os modelos de caixa branca são modelos de ML que fornecem resultados compreensíveis por especialistas no domínio. Os modelos de caixa preta, por outro lado, são extremamente difíceis de explicar e dificilmente podem ser entendidos mesmo por especialistas no domínio. Considera-se que os algoritmos de IAE seguem três princípios: transparência, interpretabilidade e explicabilidade. A transparência está presente “se os processos que extraem parâmetros de modelo de dados de treinamento e geram rótulos de dados de teste podem ser descritos e motivados pelo designer de abordagem”. A interpretabilidade descreve a possibilidade de compreender o modelo de ML e de apresentar a base subjacente para a tomada de decisão de uma forma que seja compreensível por humanos. A explicabilidade é um conceito reconhecido como importante, mas uma definição conjunta ainda não está disponível. Sugere-se que a explicabilidade em ML pode ser considerada como “a coleção de características do domínio interpretável, que contribuíram para que um dado exemplo produzisse uma decisão (por exemplo, classificação ou regressão)”. Se os algoritmos atendem a esses requisitos, eles fornecem uma base para justificar decisões, rastreá-las e, portanto, verificá-las, melhorar os algoritmos e explorar novos fatos.
Às vezes, também é possível obter um resultado com alta precisão com um algoritmo de ML de caixa branca que pode ser interpretado por si mesmo. Isso é especialmente importante em domínios como medicina, defesa, finanças e direito, onde é crucial entender as decisões e construir confiança nos algoritmos.
Os sistemas de IA otimizam o comportamento para satisfazer um sistema de objetivos especificado matematicamente, escolhido pelos projetistas do sistema, como o comando "maximizar a precisão da avaliação de quão positivas são as críticas de filmes no conjunto de dados de teste". A IA pode aprender regras gerais úteis do conjunto de testes, como "avaliações que contenham a palavra 'horrível' provavelmente serão negativas". No entanto, ela também pode aprender regras inadequadas, como "resenhas contendo 'Daniel Day-Lewis' geralmente são positivas"; tais regras podem ser indesejáveis se forem consideradas susceptíveis de falhar na generalização fora do conjunto de teste, ou se as pessoas considerarem que a regra "trapaceia" ou é "injusta". Um ser humano pode auditar regras em um XAI para ter uma ideia da probabilidade de o sistema generalizar para dados futuros do mundo real fora do conjunto de teste.