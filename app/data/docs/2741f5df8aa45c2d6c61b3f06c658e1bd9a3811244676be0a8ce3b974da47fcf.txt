Modelos de linguagem de grande escala (em inglês:  Large Language Model ou LLM) são modelos de linguagem compostos por uma rede neural com muitos parâmetros (tipicamente bilhões ou possivelmente mais). São treinados com grandes quantidades de textos não rotulado usando aprendizado de máquina não-supervisionado. Os LLM surgiram por volta de 2018, com o modelo BERT. Estes têm bom desempenho em uma ampla variedade de tarefas. Isso mudou o foco da pesquisa em processamento de linguagem natural, afastando-se do paradigma anterior de treinar modelos supervisionados especializados para tarefas específicas.