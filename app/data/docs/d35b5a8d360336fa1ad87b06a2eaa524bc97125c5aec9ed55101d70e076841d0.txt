Na inteligência artificial (IA), uma alucinação ou alucinação artificial é uma resposta confiante por parte da IA que não parece ser justificada pelos dados de treinamento.
Por exemplo, um chatbot alucinatório sem conhecimento sobre a receita de uma empresa de grande porte pode internamente escolher um número aleatório (como "US$ 13.6 bilhões") que o chatbot considere plausível, e então continuar a insistir falsa e repetidamente que a receita da referida empresa é de US$ 13.6 bilhões, sem sinal de consciência interna de que o número foi produto da própria imaginação deste.
Esses fenômenos são chamados de "alucinações", em analogia com o fenômeno da alucinação na psicologia humana. Note que enquanto uma alucinação humana é uma percepção por um ser humano que não pode ser sensatamente associada com a parte do mundo externo que o humano está diretamente observando com seus órgãos sensoriais, uma alucinação da IA é, em vez disso, uma resposta confiante que não pode ser fundamentada em nenhum dos seus dados de treinamento da mesma.
A alucinação de inteligência artificial ganhou destaque por volta de 2022 com o lançamento de certos modelos de linguagem grande (LLMs) como o ChatGPT. Os usuários reclamaram que tais bots muitas vezes pareciam incorporar sociopaticamente e sem sentido falsidades plausíveis dentro do conteúdo gerado. Outro exemplo de alucinação na inteligência artificial é quando a IA ou chatbot esquece que é uma máquina e alega ser humano.
Em 2023, os analistas consideraram a alucinação frequente um grande problema na tecnologia LLM.